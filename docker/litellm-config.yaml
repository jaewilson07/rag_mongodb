# LiteLLM config for Claude Code + vLLM on 48GB (2x 3090)
# Cap max_tokens so 20k input + output fits in vLLM's 32k context
# See: https://dev.to/dcruver/running-claude-code-with-local-llms-via-vllm-and-litellm-599b

model_list:
  - model_name: claude-*
    litellm_params:
      model: openai/glm-4.7
      api_base: http://vllm-glm:8000/v1
      api_key: "not-needed"
      max_tokens: 12000
    model_info:
      max_tokens: 12000
      max_input_tokens: 32768
      max_output_tokens: 12000
  - model_name: glm-4.7
    litellm_params:
      model: openai/glm-4.7
      api_base: http://vllm-glm:8000/v1
      api_key: "not-needed"
      max_tokens: 12000
    model_info:
      max_tokens: 12000
      max_input_tokens: 32768
      max_output_tokens: 12000

litellm_settings:
  drop_params: true
  modify_params: true
  request_timeout: 600

general_settings:
  disable_key_check: true
